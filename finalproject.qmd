---
  title: "Final Project"
author: "Jane Finocharo, Adriana Vance, and Ruining Zheng"
format: html
editor: visual
---
  
```{r}
library(tidyverse)
library(here)
library(readxl)
library(tidyr)
library(lubridate)
library(DescTools)
library(tidymodels)
library(gridExtra)
```

### Loading, Cleaning and Merging Datasets

We might want to find an easier way to load in the data, but for now I just created a data folder in my directory, and named the files CHR.csv, FEA.csv, FEA_supplement.csv, and snap.xlsx

How to access the data:
  
  \- **County Health Rankings**: At [this link](https://www.countyhealthrankings.org/health-data/methodology-and-sources/data-documentation), click "2024 CHR CSV Analytic Data". Rename file "CHR.csv"

\- **Food Environment Atlas**: At [this link](https://www.ers.usda.gov/data-products/food-environment-atlas/data-access-and-documentation-downloads/#Current%20Version), click "Food Environemnt Atlas .csv Files." Open the .zip file, move "StateAndCountyData.csv" into data folder, rename "FEA.csv". Move "SupplementalDataCounty.csv" into data folder, rename "FEA_supplement.csv". Move "VariableList.csv" into the data folder.
                                                
\- **SNAP Policy Data:** At [this link](https://www.ers.usda.gov/data-products/snap-policy-data-sets/), click "Snap Policy Database." Move into data folder, rename "snap.xlsx"
                                              
```{r}
                                              
# county health rankings
chr_col_names <- names(read_csv(here("data", "CHR.csv"), n_max = 0))
county_health_rankings <- read_csv(here("data", "CHR.csv"), col_names = chr_col_names, skip = 2)

# food atlas + food atlas supplement
food_atlas <- read_csv(here("data", "FEA.csv"))
food_atlas_supplement <- read_csv(here("data", "FEA_supplement.csv"))
atlas_vars <- read_csv(here("data", "VariableList.csv"))

# SNAP policy data
snap <- read_excel(here("data", "snap.xlsx"), sheet = 2)

```
                                              
The county health rankings dataset has over 700 variables. We need to pick which ones we'd like to take as health measures of our counties. The other problem is understanding which years these datapoints come from, because it seems to differ by measure. Variables from county health rankings data that may be helpful:

-   v133_rawvalue, v133_numerator, v133_denominator (food environment index)
-   v132_rawvalue, v132_numerator, v132_denominator (access to exercise opportunities)
-   v132_rawvalue, v132_numerator, v132_denominator (income inequality)
-   v024_rawvalue, v024_numerator, v024_denominator (children in poverty)
-   v125_rawvalue, v125_numerator, v125_denominator (air pollution particulate matter)
-   v147_rawvalue, v147_numerator, v147_denominator (life expectancy)
-   v139_rawvalue, v139_numerator (food insecurity)
-   v083_rawvalue, v083_numerator, v083_denominator (ltd. access to healthy foods)
-   v003_rawvalue, v003_numerator, v003_denominator (uninsured adults)
-   v003_rawvalue, v003_numerator, v003_denominator (uninsured children)
-   v021_rawvalue, v021_numerator, v021_denominator (HS graduation)
-   v171_rawvalue (childcare cost burdened)
-   v156_rawvalue (traffic volume)
-   v058_rawvalue, v058_numerator, v058_denominator (rural)

The food environment atlas has various variables measuring access to food and other things, but the data is not tidy. Needs to be reorganized. The supplement contains population estimates. May be helpful as controls? - Might be helpful to select which food atlas variables we want to keep before tidying the data. That way we can filter beforehand.

The SNAP policy dataset is tidy and organized by state. Easy to interpret, but the observations are by yearmonth, meaning there are 12 observations for every year of the data. Not sure how to navigate this.

Challenges/to do: Clean datasets, understand what year all the variables come from, see if it is possible to merge datasets by county and understand which counties have changed overtime. [This link](https://www.census.gov/programs-surveys/geography/technical-documentation/county-changes.2010.html#list-tab-957819518) could be helpful for knowing which county boundaries have changed overtime.

### Tidying the Datasets for Merging

1.  Each variable must form a column
2.  Each observation must form a row
3.  Each type of observational unit forms a dataframe.

#### Food Atlas Dataframe

```{r}

# Changing values from scientific notation, and rounding decimals to be shorter
food_atlas <- food_atlas %>%
  mutate(Value = format(Value, scientific = FALSE)) %>%
  mutate(Value = map_dbl(food_atlas$Value, round, digits = 4))

# Pivoting dataset to tidy - variables as columns and counties as rows 

atlas_test <- food_atlas %>%
  pivot_wider(id_cols  = c(FIPS), # ID col is FIPS because of identical county names
              names_from = c(Variable_Code), 
              values_from = Value) %>%
  filter(FIPS > 1000) 

# There are two more variables than counties. The two observations here have NA values for all variables so will be filtered from the data
atlas_test %>%
  filter(is.na(atlas_test$LACCESS_POP10))

#Filtering here
atlas_test <- atlas_test %>%
  filter(FIPS != 2158 & FIPS != 46102)

# Create a tibble with unique state-county FIPS combos 
strings <- c("County", "Total", "Parish", "city", "Borough", "Census Area", "Municipality")
fips_distinct <- food_atlas %>%
  group_by(FIPS) %>%
  select(State, County, FIPS) %>%
  distinct() %>%
  filter(!str_detect(County, pattern = paste(strings, collapse = "|")))

# Bind dataframes with data and county/state names

food_atlas <- full_join(fips_distinct, atlas_test)

```

Here, we pivoted the data to be wider such that each unique county name formed a row. To do so, we used the FIPS codes which are unique identifiers of each county. After pivoting, we noticed there were state-level variables and duplicates of each county. We filtered out any county duplicates using stringr, and created a dataframe of unique state, county, and FIPS codes to join with the final dataset. The final dataset has 3,143 unique observations, which is the number of counties in the U.S.

#### SNAP Dataframe

```{r}

# Round the decimals to be the same pattern as above
snap <- snap %>%
  mutate_if(is.numeric, ~ round(., digits = 4)) %>%
  filter(yearmonth >= 201001) %>% # Only keep the data after year 2010. to assure that our analysis is closely connected to the current policies and circumstances.
  mutate(yearmonth = ym(yearmonth)) %>%
  mutate(year = year(yearmonth)) # Since the CHR data only contains year data, we also mutate a new column for year in SNAP. 

snap_natest <- snap %>%
  select(-transben, -call_any, -faceini, -facerec, -(starts_with("cert"))) %>% # Removes variables with NA values for multiple years
  na.omit() # Removes any NA observations left, mostly in 2020 due to pandemic 

# Function to change year-month observations into year observations, taking the most common value 
#' Year-Month -> Year
#'
#' @param date # This is actually for the year of the dataset. Used to filter dataset from years 2010-2020
#' @param yearname # This is also the year, but must be written in quotes. Ex: yearname = "2016" 
#'
#' @return # Returns a dataframe of a specific year of the SNAP data. Each observation is the mode of year-month observations in the original data. 
#' @export
#'
#' @examples
#' snap_2010 <- rename_columns(date = 2010, yearname = "_2010")

rename_columns <- function(date, yearname){
  snap_date <- snap_natest %>%
    filter(year == date) %>%
    group_by(state_pc, year) %>%
    summarize_all(list(mode = ~Mode(.))) %>%
    select(-state_fips_mode, -statename_mode, -yearmonth_mode) %>%
    distinct(state_pc, .keep_all = TRUE)  # Keep only one row per state_pc

  names_date <- names(snap_date)
  new_date <- paste(names_date, yearname, sep = "")
  names(snap_date) <- new_date

  return(snap_date)
}
snap_2010 <- rename_columns(date = 2010, yearname = "_2010")
snap_2011 <- rename_columns(date = 2011, yearname = "_2011")
snap_2012 <- rename_columns(date = 2012, yearname = "_2012")
snap_2013 <- rename_columns(date = 2013, yearname = "_2013")
snap_2014 <- rename_columns(date = 2014, yearname = "_2014")
snap_2015 <- rename_columns(date = 2015, yearname = "_2015")
snap_2016 <- rename_columns(date = 2016, yearname = "_2016")
snap_2017 <- rename_columns(date = 2017, yearname = "_2017")
snap_2018 <- rename_columns(date = 2018, yearname = "_2018")
snap_2019 <- rename_columns(date = 2019, yearname = "_2019")
snap_2020 <- rename_columns(date = 2020, yearname = "_2020")

snap_year <- bind_cols(snap_2010, 
                       snap_2011, 
                       snap_2012, 
                       snap_2013, 
                       snap_2014, 
                       snap_2015, 
                       snap_2016, 
                       snap_2017, 
                       snap_2018, 
                       snap_2019, 
                       snap_2020) %>%
  rename(state = state_pc_2010) %>%
  group_by(state) %>%
  select(-starts_with("state_pc"), 
         -starts_with("year"))

```

The SNAP dataframe is a state-level dataframe of monthly reported data of SNAP policy relevant information by each state from 1996 to 2019. It supposed to have 51*12*10 = 6120 observations, but since we filter out data before year 2010 as well as states' data with too many missing values, the actual observation number is smaller.

Through looking at the documentation for the SNAP policy data, we learned that some variables were systematically missing. There were a few variables (transben, call_any, faceini, facerec, and the recertification variables) that had missing data for multiple years. Because the variables were missing not at random, we decided to remove them from our analysis. The rest of the variables were missing data from 2020, which is likely due to lack of data collection during the pandemic. NA observations were removed because imputation would likely not be able to reflect policy changes made during the pandemic. 

After dealing with the missing values, we grouped the dataset by year and state, and took the mode of each observation. The data was categorical, so mode was used to capture the most frequent instance of each policy during each year. 

#### County Health Rankings

```{r}

# selecting and renaming key variables. adding _chr at end so we know which variables came from CHR in merged dataset

chr_small <- 
  county_health_rankings |> 
  select(state_fips = 'State FIPS Code',
         county_fips = 'County FIPS Code',
         state = 'State Abbreviation',
         fips_5_digit = '5-digit FIPS Code',
         county = 'Name',
         food_env_index_chr = 'Food Environment Index raw value',
         exercise_access_chr = 'Access to Exercise Opportunities raw value',
         child_poverty_chr = 'Children in Poverty raw value',
         income_inequality_chr = 'Income Inequality raw value',
         air_pollution_chr = 'Air Pollution - Particulate Matter raw value',
         limited_healthy_food_chr = 'Limited Access to Healthy Foods raw value',
         uninsured_adults_chr = 'Uninsured Adults raw value',
         uninsured_children_chr = 'Uninsured Children raw value',
         childcare_cost_burden_chr = 'Child Care Cost Burden raw value',
         traffic_volume_chr = 'Traffic Volume raw value',
         rural_chr = '% Rural raw value', 
         hs_grad_rate_chr = 'High School Graduation raw value',
         food_insecurity_chr = 'Food Insecurity raw value',
         life_expectancy_chr = 'Life Expectancy raw value',
         pct_under18_chr = '% Below 18 Years of Age raw value',
         housing_cost_burden_chr = 'Severe Housing Cost Burden raw value',
         residential_segregation_chr = 'Residential Segregation - Black/White raw value',
         pct_NH_Black_chr = '% Non-Hispanic Black raw value',
         pct_Hispanic_chr = '% Hispanic raw value',
         pct_NH_white_chr = '% Non-Hispanic White raw value',
         pct_not_english_prof_chr = '% Not Proficient in English raw value') |> 
  filter(county_fips!="000") |> 
  mutate(
    fips = as.numeric(fips_5_digit)) |> 
  select(!c(state_fips, county_fips, fips_5_digit)) |> 
  mutate_if(is.numeric, ~ round(., digits = 4))

glimpse(chr_small)
```

### Merging Data

```{r}

# merging atlas and CHR
food_atlas <- food_atlas %>%
  rename(fips = FIPS)

anti_join(food_atlas, chr_small, by = "fips")
anti_join(chr_small, food_atlas)

```
The anti joins above show the counties in each dataset that are not represented in the other. All of these counties were either changed or deleted in the time that these data were collected. In order to be able to merge our datasets, we will remove these counties from the data. 

```{r}

food_atlas <- food_atlas %>%
  filter(fips != 2261, 
         fips != 2270, 
         fips != 46113, 
         fips != 51515)
chr_small <- chr_small %>%
  filter(fips != 2063,
         fips != 2066, 
         fips != 2158, 
         fips != 46102)

# Checking that there are no other unmatched observations 
anti_join(food_atlas, chr_small, by = "fips") # success!
anti_join(chr_small, food_atlas, by = "fips") # success!

atlas_chr <- full_join(food_atlas, chr_small, by = "fips") 

snap_year <- snap_year %>%
  rename(State = state)

atlas_chr_snap <- left_join(atlas_chr, snap_year)

```

### Predictive Model

#### Problem Formulation

This predictive model aims to predict the food security and health conditions in the county level, through the data of access to healthy food. 

#### Split data into training and testing data

```{r}
split <- initial_split(data = atlas_chr_snap, prop = 0.75)

training <- training(split) 
testing <- testing(split)

```

#### Exploratory Data Analysis

```{r}

training %>%
  ungroup() %>%
  reframe(
    mean = mean(POVRATE15),
    median = median(POVRATE15),
    range = range(POVRATE15),
    qs = quantile(POVRATE15, c(0.25, 0.75)), prob = c(0.25, 0.75)
    )

training %>%
  ggplot(mapping = aes(x = POVRATE15, y = food_insecurity_chr, color = POVRATE15)) +
  geom_point() +
  geom_smooth(method = "lm", se = TRUE) +
  labs(title = "Relationship between poverty rate and food insecurity value",
       x = "Poverty rate",
       y = "Food insecurity value") + 
  theme_minimal()

```
The line chart shows a general linear relationship between the poverty rate of each county and the food insecurity value. As the poverty rate increases, the food insecurity value also increases. 

We also think the food security is closely connected to the programs and policies participants since the purposes of these benefits are to help people who are in danger of food insecurity due to various reasons. Thus, we selected the program participant proportional variables and observe the density distribution of them and of food security value. 

```{r}

program_participants <- training %>%
  select(
    fips,
    PCT_SNAP17, 
    PCT_NSLP17, 
    PCT_SBP17, 
    PCT_SFSP17, 
    PCT_WIC17, 
    PCT_WICINFANTCHILD16,
    PCT_WICWOMEN16,
    food_insecurity_chr
  )

# Density distribution of food insecurity value among all observations
program_participants %>%
  ggplot() +
  geom_density(mapping = aes(x = food_insecurity_chr), color = "indianred") +
  labs(title = "Density plot for food insecurity score") +
  theme_minimal()

# Density distribution of program participant proportions among all observations

p1 <- program_participants %>%
  ggplot() +
  geom_density(mapping = aes(x = PCT_WIC17), color = "purple") +
  labs(title = "WIC") +
  theme_minimal()

p2 <- program_participants %>%
  ggplot() +
  geom_density(mapping = aes(x = PCT_WICINFANTCHILD16), color = "orange") +
  labs(title = "WIC infant and children") +
  theme_minimal()

p3 <- program_participants %>%
  ggplot() +
  geom_density(mapping = aes(x = PCT_WICWOMEN16), color = "olivedrab") +
  labs(title = "WIC women") +
  theme_minimal()
 
p4 <- program_participants %>%
  ggplot() +
  geom_density(mapping = aes(x = PCT_SNAP17), color = "gold") +
  labs(title = "SNAP") +
  theme_minimal()

p5 <- program_participants %>%
  ggplot() +
  geom_density(mapping = aes(x = PCT_NSLP17), color = "dodgerblue") +
  labs(title = "National School Lunch Program") +
  theme_minimal()

p6 <- program_participants %>%
  ggplot() +
  geom_density(mapping = aes(x = PCT_SBP17), color = "green") +
  labs(title = "School Breakfast Program") +
  theme_minimal()

grid.arrange(p1, p2, p3, p4, p5, p6, ncol = 3)

```
The density distribution of the program participation proportions do not show the similar trends as we expected before, so we will do the correlation test to determine whether these variables will be included as predictors. 

#### Feature Selection
```{r}

correlation <- training %>%
  select(where(is.numeric)) %>%
  cor()

# Only keep variables have correlation with the food insecurity variable. 
correlation_df <- as.data.frame(correlation) %>%
  select(food_insecurity_chr) %>%
  filter(food_insecurity_chr != 0) %>% # filter out those correlations equal 0
  rownames_to_column() %>%
  pivot_wider(
    names_from = rowname,
    values_from = food_insecurity_chr
  )

# Intersect correlation data frame with training data, so only variables show in both data frame will be left. 
selected_variables <- intersect(names(correlation_df), names(training)) 
as.data.frame(selected_variables)

# Filter out variables that are not in intersection results as the last step assigned to
sub_training <- training[selected_variables]

```
By doing this, we narrow down the number of predictors into 362 variables instead of 604 variables in the original training data. 

### Visualizing Correlations 
```{r}

c1 <- training %>%
  ggplot(mapping = aes(x = food_insecurity_chr, y = SNAPSPTH17)) + 
  geom_point() + 
  geom_smooth()

c2 <- training %>%
  ggplot(mapping = aes(x = food_insecurity_chr, y = SNAPSPTH12)) + 
  geom_point() + 
  geom_smooth()

c3 <- training %>%
  ggplot(mapping = aes(x = food_insecurity_chr, y = LACCESS_POP10)) + 
  geom_point() + 
  geom_smooth()

c4 <- training %>%
  ggplot(mapping = aes(x = food_insecurity_chr, y = GROC16)) + 
  geom_point() + 
  geom_smooth()

c5 <- training %>%
  ggplot(mapping = aes(x = food_insecurity_chr, y = SUPERCPTH16)) + 
  geom_point() + 
  geom_smooth()

grid.arrange(c1, c2, c3, c4, c5, ncol = 3)

sub_training %>%
  mutate(log_pop = log(Population_Estimate_2018)) %>%
  ggplot(mapping = aes(x = log_pop)) +
  geom_density()

```

#### Set up Resampling

```{r}
sub_folds <- vfold_cv(data = sub_training, v = 10, repeats = 10, strata = Population_Estimate_2018)

```

#### Lasso Regression

```{r}

lasso_rec <- recipe(food_insecurity_chr ~ ., data = sub_training) %>%
  add_role(fips, new_role = "id") %>%
  step_rm(has_role("id")) %>%
  step_nzv(all_predictors()) %>%
  step_impute_mean(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>% 
  step_other(all_nominal(), threshold = 0.05)
  
lasso_mod <- linear_reg(penalty = tune(), mixture = 1) %>%
  set_mode(mode = "regression") %>%
  set_engine(engine = "glmnet")
  
lasso_wf <- workflow() %>%
  add_recipe(recipe = lasso_rec) %>%
  add_model(spec = lasso_mod)

penalty_grid <- grid_regular(penalty(), levels = 20)

lasso_res <- lasso_wf %>%
  tune_grid(
    resamples = sub_folds,
    grid = penalty_grid,
    metrics = metric_set(rmse),
    control = control_grid(verbose = FALSE, save_pred = TRUE)
  )

lasso_res %>%
  collect_metrics()

```

#### Random Forest

```{r}

rf_rec <- recipe(food_insecurity_chr ~ ., sub_training) %>%
  add_role(fips, new_role = "id") %>%
  step_rm(has_role("id")) %>%
  step_nzv(all_predictors()) %>%
  step_normalize(all_numeric_predictors()) %>%
  step_other(all_nominal(), threshold = 0.05)

rf_mod <- rand_forest(
  trees = 200,
  mtry = 14,
  min_n = 5
) %>%
  set_mode(mode = "regression") %>%
  set_engine(
    engine = "ranger", 
    importance = "impurity",
    num.threads = 4
  )

rf_wf <- workflow() %>%
  add_recipe(rf_rec) %>%
  add_model(rf_mod)

rf_res <- rf_wf %>%
  fit_resamples(resamples = sub_folds)

rf_res %>%
  collect_metrics() 

```

